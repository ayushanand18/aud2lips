{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from glob import glob\nimport numpy as np\nfrom matplotlib import pylab as plt\nimport cv2\nimport time\nimport os\nimport torchvision.transforms as transforms\nimport torch\nimport tensorflow as tf\nprint(tf.__version__)\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Model\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:54:33.275429Z","iopub.execute_input":"2022-02-10T10:54:33.275711Z","iopub.status.idle":"2022-02-10T10:54:40.593543Z","shell.execute_reply.started":"2022-02-10T10:54:33.275682Z","shell.execute_reply":"2022-02-10T10:54:40.592818Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/videopaths/datapaths.csv',header=0,index_col='Unnamed: 0')\ndf.replace('audio-files-for-miraclvc1/ph1-13','audio-files-for-miraclvc1/ph1-1',inplace=True)\nfor i in list(set(df['spectrograms'])):\n    if not os.path.exists('../input/'+i+'.jpg'):\n        df.replace(i,'audio-files-for-miraclvc1/ph1-1',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:54:40.595060Z","iopub.execute_input":"2022-02-10T10:54:40.595585Z","iopub.status.idle":"2022-02-10T10:54:42.414534Z","shell.execute_reply.started":"2022-02-10T10:54:40.595553Z","shell.execute_reply":"2022-02-10T10:54:42.413749Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Aud2LipsDataset(torch.utils.data.Dataset):\n    def __init__(self):\n        self.imgs = df\n    def __getitem__(self, idx):\n        # load lipimage,spectrogram and target:lipmovement image\n        lip_path = self.imgs['lippath'][idx]\n        spect_path='../input/'+self.imgs['spectrograms'][idx]+'.jpg'\n        lipmove_path=self.imgs['videopath'][idx]\n        #resize and load as tensor\n        lipimg=cv2.cvtColor(cv2.imread(lip_path), cv2.COLOR_BGR2RGB)\n        spectimg=cv2.cvtColor(cv2.imread(spect_path),cv2.COLOR_BGR2RGB)\n        lipmoveimg=cv2.cvtColor(cv2.imread(lipmove_path), cv2.COLOR_BGR2RGB)\n        lipimg = lipimg[250:400,150:400]\n        lipimg = cv2.resize(lipimg,(170,150))\n        spectT = cv2.resize(spectimg,(80, 150))\n        lipmoveimg=lipmoveimg[250:400,150:400]\n        imageA = np.concatenate((lipimg,spectT),axis=1)\n        imageB = lipmoveimg\n        return imageA,imageB\n    def __len__(self):\n        return len(self.imgs)\ndataset = Aud2LipsDataset()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:54:42.415892Z","iopub.execute_input":"2022-02-10T10:54:42.416145Z","iopub.status.idle":"2022-02-10T10:54:42.425365Z","shell.execute_reply.started":"2022-02-10T10:54:42.416108Z","shell.execute_reply":"2022-02-10T10:54:42.424542Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_batch(batch_size):\n    n_batches=int(len(df)/batch_size)\n    for i in range(n_batches):\n        img_A,img_B=[],[]\n        for idx in range(i*batch_size,(i+1)*batch_size):\n            img1,img2 = dataset[idx]\n            img_A.append(img1)#picture\n            img_B.append(img2)#label\n      \n        img_A=np.array(img_A)/127.5-1\n        img_B=np.array(img_B)/127.5-1\n    \n        yield img_A,img_B #return generator","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:54:53.878334Z","iopub.execute_input":"2022-02-10T10:54:53.878844Z","iopub.status.idle":"2022-02-10T10:54:53.884658Z","shell.execute_reply.started":"2022-02-10T10:54:53.878805Z","shell.execute_reply":"2022-02-10T10:54:53.883965Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class pix2pix():\n    def __init__(self):\n        self.img_rows= 150\n        self.img_cols= 250\n        self.channels=3\n        self.img_shape=(self.img_rows,self.img_cols,self.channels)\n    \n        patch=int(self.img_rows/(2**4)) # 16\n        self.disc_patch=(10,16,1)\n    \n        self.gf=64\n        self.df=64\n    \n        optimizerD = tf.keras.optimizers.Adam(0.0004,0.5)\n        optimizerG = tf.keras.optimizers.Adam(0.0001,0.5)\n        \n        self.discriminator=self.build_discriminator()\n        #self.discriminator.summary()\n        self.discriminator.compile(loss='binary_crossentropy',\n                              optimizer=optimizerD)\n    \n        self.generator=self.build_generator()\n        #self.generator.summary()\n    \n        img_A=layers.Input(shape=self.img_shape)#picture--label\n        img_B=layers.Input(shape=self.img_shape)#label--real\n    \n        img=self.generator(img_A)\n    \n        self.discriminator.trainable=False\n    \n        valid=self.discriminator([img,img_A])\n    \n        self.combined=Model(img_A,valid)\n        self.combined.compile(loss='binary_crossentropy',\n                              optimizer=optimizerG)\n    \n    def build_generator(self):\n        def conv2d(layer_input,filters,f_size=(4,4),bn=True):\n            d=layers.Conv2D(filters,kernel_size=f_size,strides=(2,2),padding='same')(layer_input)\n            d=layers.LeakyReLU(0.2)(d)\n            if bn:\n                d=layers.BatchNormalization()(d)\n            return d\n    \n        def deconv2d(layer_input,skip_input,filters,f_size=(4,4),dropout_rate=0):\n            u=layers.UpSampling2D((2,2))(layer_input)\n            u=layers.Conv2D(filters,kernel_size=f_size,strides=(1,1),padding='same',activation='relu')(u)\n            if dropout_rate:\n                u=layers.Dropout(dropout_rate)(u)\n            u=layers.BatchNormalization()(u)\n            u=layers.Concatenate()([u,skip_input])\n            return u\n    \n        d0=layers.Input(shape=self.img_shape)\n    \n        d1=conv2d(d0,self.gf,bn=False) \n        d1 = tf.keras.layers.Resizing(256,320)(d1)\n        d2=conv2d(d1,self.gf*2)\n        d2 = tf.keras.layers.Resizing(128,160)(d2)\n        d3=conv2d(d2,self.gf*4)  \n        d3 = tf.keras.layers.Resizing(64,80)(d3)\n        d4=conv2d(d3,self.gf*8) \n        d4 = tf.keras.layers.Resizing(32,40)(d4)\n        d5=conv2d(d4,self.gf*8)\n        d5 = tf.keras.layers.Resizing(16,20)(d5)\n        d6=conv2d(d5,self.gf*8)        \n    \n        d7=conv2d(d6,self.gf*8)         \n    \n        u1=deconv2d(d7,d6,self.gf*8,dropout_rate=0.5)\n        u2=deconv2d(u1,d5,self.gf*8,dropout_rate=0.5)   \n        u3=deconv2d(u2,d4,self.gf*8,dropout_rate=0.5)   \n        u4=deconv2d(u3,d3,self.gf*4)   \n        u5=deconv2d(u4,d2,self.gf*2)   \n        u6=deconv2d(u5,d1,self.gf)     \n        u7=layers.UpSampling2D((2,2))(u6)\n    \n        output_img=layers.Conv2D(self.channels,kernel_size=(4,4),strides=(1,1),padding='same',activation='tanh')(u7)\n        output_img = tf.keras.layers.Resizing(150,250)(output_img)\n        \n        return Model(d0,output_img)\n  \n    def build_discriminator(self):\n        def d_layer(layer_input,filters,f_size=(4,4),bn=True):\n            d=layers.Conv2D(filters,kernel_size=f_size,strides=(2,2),padding='same')(layer_input)\n            d=layers.LeakyReLU(0.2)(d)\n            if bn:\n                d=layers.BatchNormalization()(d)\n            return d\n    \n        img_A=layers.Input(shape=self.img_shape)\n        img_B=layers.Input(shape=self.img_shape)\n    \n        combined_imgs=layers.Concatenate(axis=-1)([img_A,img_B])\n    \n        d1=d_layer(combined_imgs,self.df,bn=False)\n        d2=d_layer(d1,self.df*2)\n        d3=d_layer(d2,self.df*4)\n        d4=d_layer(d3,self.df*8)\n    \n        validity=layers.Conv2D(1,kernel_size=(4,4),strides=(1,1),padding='same',activation='sigmoid')(d4)\n    \n        return Model([img_A,img_B],validity)\n  \n    def train(self,epochs,batch_size=1):\n        valid=np.ones((batch_size,)+self.disc_patch)\n        fake=np.zeros((batch_size,)+self.disc_patch)\n    \n        for epoch in range(epochs):\n            start=time.time()\n            for batch_i,(img_A,img_B) in enumerate(load_batch(batch_size)):\n                gen_imgs=self.generator.predict(img_A)\n        \n                d_loss_real = self.discriminator.train_on_batch([img_B, img_A], valid)\n                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, img_A], fake)\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        \n                g_loss = self.combined.train_on_batch(img_A,valid)\n                g_loss = self.combined.train_on_batch(img_A,valid)\n\n                if batch_i % 50 == 0:\n                    print (\"[Epoch %d] [Batch %d] [D loss: %f] [G loss: %f]\" % (epoch,\n                                                                                batch_i,\n                                                                                d_loss,\n                                                                                g_loss))\n                if batch_i % 1000 == 0:\n                    tf.keras.models.save_model(self.generator, 'trained-gan-model',\n                                               overwrite=True, include_optimizer=True,\n                                               save_format=None)\n            self.sample_images(epoch)\n            print('Time for epoch {} is {} sec'.format(epoch,time.time()-start))\n      \n    def sample_images(self, epoch):\n        (o,t) = dataset[3180]\n        o=o[None,]\n        outi_image = gan.generator.predict(o)\n        plt.imshow(outi_image.squeeze())\n        plt.savefig(\"./%d.png\" % (epoch))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T11:27:05.945350Z","iopub.execute_input":"2022-02-10T11:27:05.945693Z","iopub.status.idle":"2022-02-10T11:27:05.982532Z","shell.execute_reply.started":"2022-02-10T11:27:05.945659Z","shell.execute_reply":"2022-02-10T11:27:05.981795Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Loading Model","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    gan = pix2pix()\n    gan.generator = tf.keras.models.load_model('../input/gan-aud2lips-v2/trained-gan-model/', compile=True)\n    gan.train(1,4)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T11:27:08.171757Z","iopub.execute_input":"2022-02-10T11:27:08.172011Z","iopub.status.idle":"2022-02-10T11:29:26.718266Z","shell.execute_reply.started":"2022-02-10T11:27:08.171983Z","shell.execute_reply":"2022-02-10T11:29:26.717082Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"(o,t) = dataset[0]\no=o[None,]\nout_image = gan.generator.predict(o)\nplt.imshow(out_image.squeeze())","metadata":{"execution":{"iopub.status.busy":"2022-02-10T11:30:05.341799Z","iopub.execute_input":"2022-02-10T11:30:05.342422Z","iopub.status.idle":"2022-02-10T11:30:05.692160Z","shell.execute_reply.started":"2022-02-10T11:30:05.342349Z","shell.execute_reply":"2022-02-10T11:30:05.691458Z"},"trusted":true},"execution_count":37,"outputs":[]}]}